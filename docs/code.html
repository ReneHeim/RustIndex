<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">

<head>

<meta charset="utf-8" />
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="pandoc" />




<title>Code</title>

<script src="site_libs/jquery-1.11.3/jquery.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="site_libs/bootstrap-3.3.5/css/flatly.min.css" rel="stylesheet" />
<script src="site_libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<script src="site_libs/jqueryui-1.11.4/jquery-ui.min.js"></script>
<link href="site_libs/tocify-1.9.1/jquery.tocify.css" rel="stylesheet" />
<script src="site_libs/tocify-1.9.1/jquery.tocify.js"></script>
<script src="site_libs/navigation-1.1/tabsets.js"></script>
<link href="site_libs/highlightjs-9.12.0/default.css" rel="stylesheet" />
<script src="site_libs/highlightjs-9.12.0/highlight.js"></script>
<link href="site_libs/font-awesome-5.0.13/css/fa-svg-with-js.css" rel="stylesheet" />
<script src="site_libs/font-awesome-5.0.13/js/fontawesome-all.min.js"></script>
<script src="site_libs/font-awesome-5.0.13/js/fa-v4-shims.min.js"></script>

<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
  pre:not([class]) {
    background-color: white;
  }
</style>
<script type="text/javascript">
if (window.hljs) {
  hljs.configure({languages: []});
  hljs.initHighlightingOnLoad();
  if (document.readyState && document.readyState === "complete") {
    window.setTimeout(function() { hljs.initHighlighting(); }, 0);
  }
}
</script>



<style type="text/css">
h1 {
  font-size: 34px;
}
h1.title {
  font-size: 38px;
}
h2 {
  font-size: 30px;
}
h3 {
  font-size: 24px;
}
h4 {
  font-size: 18px;
}
h5 {
  font-size: 16px;
}
h6 {
  font-size: 12px;
}
.table th:not([align]) {
  text-align: left;
}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />

</head>

<body>

<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
code {
  color: inherit;
  background-color: rgba(0, 0, 0, 0.04);
}
img {
  max-width:100%;
  height: auto;
}
.tabbed-pane {
  padding-top: 12px;
}
.html-widget {
  margin-bottom: 20px;
}
button.code-folding-btn:focus {
  outline: none;
}
</style>


<style type="text/css">
/* padding for bootstrap navbar */
body {
  padding-top: 60px;
  padding-bottom: 40px;
}
/* offset scroll position for anchor links (for fixed navbar)  */
.section h1 {
  padding-top: 65px;
  margin-top: -65px;
}

.section h2 {
  padding-top: 65px;
  margin-top: -65px;
}
.section h3 {
  padding-top: 65px;
  margin-top: -65px;
}
.section h4 {
  padding-top: 65px;
  margin-top: -65px;
}
.section h5 {
  padding-top: 65px;
  margin-top: -65px;
}
.section h6 {
  padding-top: 65px;
  margin-top: -65px;
}
</style>

<script>
// manage active state of menu based on current page
$(document).ready(function () {
  // active menu anchor
  href = window.location.pathname
  href = href.substr(href.lastIndexOf('/') + 1)
  if (href === "")
    href = "index.html";
  var menuAnchor = $('a[href="' + href + '"]');

  // mark it active
  menuAnchor.parent().addClass('active');

  // if it's got a parent navbar menu mark it active as well
  menuAnchor.closest('li.dropdown').addClass('active');
});
</script>


<div class="container-fluid main-container">

<!-- tabsets -->
<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});
</script>

<!-- code folding -->




<script>
$(document).ready(function ()  {

    // move toc-ignore selectors from section div to header
    $('div.section.toc-ignore')
        .removeClass('toc-ignore')
        .children('h1,h2,h3,h4,h5').addClass('toc-ignore');

    // establish options
    var options = {
      selectors: "h1,h2,h3",
      theme: "bootstrap3",
      context: '.toc-content',
      hashGenerator: function (text) {
        return text.replace(/[.\\/?&!#<>]/g, '').replace(/\s/g, '_').toLowerCase();
      },
      ignoreSelector: ".toc-ignore",
      scrollTo: 0
    };
    options.showAndHide = false;
    options.smoothScroll = false;

    // tocify
    var toc = $("#TOC").tocify(options).data("toc-tocify");
});
</script>

<style type="text/css">

#TOC {
  margin: 25px 0px 20px 0px;
}
@media (max-width: 768px) {
#TOC {
  position: relative;
  width: 100%;
}
}


.toc-content {
  padding-left: 30px;
  padding-right: 40px;
}

div.main-container {
  max-width: 1200px;
}

div.tocify {
  width: 20%;
  max-width: 260px;
  max-height: 85%;
}

@media (min-width: 768px) and (max-width: 991px) {
  div.tocify {
    width: 25%;
  }
}

@media (max-width: 767px) {
  div.tocify {
    width: 100%;
    max-width: none;
  }
}

.tocify ul, .tocify li {
  line-height: 20px;
}

.tocify-subheader .tocify-item {
  font-size: 0.90em;
  padding-left: 25px;
  text-indent: 0;
}

.tocify .list-group-item {
  border-radius: 0px;
}

.tocify-subheader {
  display: inline;
}
.tocify-subheader .tocify-item {
  font-size: 0.95em;
}

</style>

<!-- setup 3col/9col grid for toc_float and main content  -->
<div class="row-fluid">
<div class="col-xs-12 col-sm-4 col-md-3">
<div id="TOC" class="tocify">
</div>
</div>

<div class="toc-content col-xs-12 col-sm-8 col-md-9">




<div class="navbar navbar-default  navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="index.html">LMMR Index</a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        <li>
  <a href="index.html">
    <span class="fas fa fas fa-clipboard"></span>
     
    Summary
  </a>
</li>
<li>
  <a href="data.html">
    <span class="fa fa-table"></span>
     
    Data
  </a>
</li>
<li>
  <a href="code.html">
    <span class="fa fa-file-code-o"></span>
     
    Code
  </a>
</li>
      </ul>
      <ul class="nav navbar-nav navbar-right">
        
      </ul>
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

<div class="fluid-row" id="header">



<h1 class="title toc-ignore">Code</h1>

</div>


<p>The following code generates the LMMR index (LemonMyrtle-MyrtleRust), a new spectral disease index for the pathosystem <em>Austropuccinia psidii</em> and <em>Backhousia citriodora</em>.</p>
<div id="setup-coding-environment" class="section level1">
<h1>Setup coding environment</h1>
<div id="install-and-load-packages" class="section level2">
<h2>Install and load packages</h2>
<p>Please run the following code to install required R packages</p>
<pre class="r"><code>install.packages(c(&quot;caret&quot;,
                   &quot;cowplot&quot;,
                   &quot;devtools&quot;,
                   &quot;gdata&quot;, 
                   &quot;ggplot2&quot;, 
                   &quot;glmulti&quot;,
                   &quot;hsdar&quot;, 
                   &quot;magrittr&quot;, 
                   &quot;plyr&quot;, 
                   &quot;PresenceAbsence&quot;, 
                   &quot;prospectr&quot;, 
                   &quot;rJava&quot;, 
                   &quot;reshape2&quot;,
                   &quot;VSURF&quot;))</code></pre>
<p><strong>Note:</strong> For the package <em>rJava</em> it is necessary to set the path of the directory where Java is installed. Please install correct version of Java (32 or 64 bit) before setting the path to the Java dir. For help follow this <a href="https://stackoverflow.com/questions/27661325/unable-to-load-rjava-on-r">LINK</a></p>
<p>For my setup I had to run:</p>
<pre class="r"><code>Sys.setenv(JAVA_HOME=&#39;C:\\Program Files\\Java\\jre1.8.0_151&#39;)</code></pre>
<p>Now we load the installed packages:</p>
<pre class="r"><code>library(caret)
library(cowplot)

library(devtools)

library(gdata)
library(ggplot2)
library(glmulti)

library(hsdar)

library(magrittr)

library(plyr)
library(PresenceAbsence)
library(prospectr)

library(rJava)
library(reshape2)

library(VSURF)</code></pre>
<p>Also the necessary functions, which can be download from the GitHub repository:</p>
<pre class="r"><code>source(&quot;R/FUN_drop_cat_var.R&quot;)
source(&quot;R/FUN_exportVSURF.R&quot;)
source(&quot;R/FUN_raw2speclibhsdar.R&quot;)
source(&quot;R/FUN_prepggwide2long.R&quot;)</code></pre>
<p>And create a directory for the analysis output:</p>
<pre class="r"><code>dir.create(&quot;output&quot;, FALSE, FALSE)</code></pre>
</div>
</div>
<div id="a" class="section level1">
<h1>A</h1>
<div id="loading-and-preparing-data" class="section level2">
<h2>Loading and preparing data</h2>
<p>The following chunk of code is loading the cleaned spectral data (1), is then checking how many factor levels are stored in the <em>Type</em> column (2) and also dropping the level <Healthy> (3). The column <em>Type</em> is assigned to a single object for later use (4). Then, the remaining factor levels are converted (5) into binary variables (1= Untreated and 0= Treated) to be used by a logistic regression function. And finally, all reflectance values are log-transformed (6) to be able to design a ratio index in subsequent steps (<strong>Note:</strong> log() is ln).</p>
<pre class="r"><code>ori.data &lt;- read.csv(&quot;data/data.wo.out.binned.cut.csv&quot;) #1

levels(ori.data$Type) #2

data &lt;- drop_class(ori.data, ori.data$Type, &quot;Healthy&quot;) #3

Type &lt;- data$Type #4

data$Type &lt;- ifelse(data$Type==&#39;Untreated&#39;,1,0) #5

data[names(data)[-1]] &lt;- log(data[names(data)[-1]]) #6</code></pre>
</div>
<div id="feature-selection" class="section level2">
<h2>Feature selection</h2>
<p>The <em>VSURF</em> package is used to conduct a pre-selection of important variables. This pre-selection was done as it reduced the total runtime of the analysis by avoiding a direct model selection procedure which would be more time consuming. The <em>VSURF</em> selection was repeated 10 times (1-2) to account for random selection encounters (~ 30h runtime).</p>
<pre class="r"><code>set.seed(20180111)

feature.set &lt;- list()
runs &lt;- seq(1, 10, 1) # 1
for (i in runs) {
  feature.set[[i]] &lt;-
    VSURF(data[, 2:202], data[, 1],
      clusterType = &quot;FORK&quot;, ntree = 2000,
      mtry = 50
    )
} # 2

saveRDS(feature.set, &quot;data/features.rds&quot;) # 4</code></pre>
<pre class="r"><code>feature.set &lt;- readRDS(&quot;data/features.rds&quot;) #5</code></pre>
<p>As we ran the <em>VSURF</em> feature selection ten times, the container list (1) was filled with ten result object where each contains, amongst others, three vectors of important variables. One vector (varselect.thres) contains a coarse set of important features, another (varselect.interp) a refined set, and a third contains a set of important features with no redundancy (varselect.pred). Pls refer to the <em>VSURF</em> vignette for more information on the different sets. For our analysis, we want to find the unique wavebands among all ten selections. Therefore a result list was created (2) and we looped through each single object (3) to assign them to the result list (band.vectors). This list was turned into a single object (4) and all unique wavebands were selected (5).</p>
<pre class="r"><code>runs &lt;- seq(1, length(feature.set), 1) # 1
band.vectors &lt;- list() # 2

for (i in runs) {
  band.vectors[[i]] &lt;-
    export_vsurf(feature.set[[i]]$varselect.pred, data[, 2:202])
} # 3

VSURF.selection &lt;- unlist(band.vectors) # 4
VSURF.selection &lt;- sort(unique(VSURF.selection)) # 5</code></pre>
<p>The 27 unique, most important wavebands could be used as input for an exhaustive model selection procedure using the package <em>glmulti</em>. We first set a seed (1) to avoid random number processing and reproduce our results and then run the model selection (2) to find the four most important wavebands. As <em>glmulti</em> does not provide model coefficients, another logistic regression was run (3) on the <em>glmulti</em> result to find coefficients for the model. We saved (4) and reloaded (5) the final model to reproduce our results quicker when re-running the code. Eventually, the coefficients (6) and the most important wavebands (7) of the final model were used to generate a linear function (8) that is most suitable to discriminate treated and untreated trees.</p>
<pre class="r"><code>set.seed(20180117) # 1

multi.model &lt;- glmulti(y = names(data)[1],
                       xr = paste0(&quot;X&quot;, VSURF.selection),
                       data, maxsize = 4,
                       level = 1,
                       family = binomial) # 2

model.1 &lt;- glm(as.formula(summary(multi.model)$bestmodel),
               data,
               family = binomial) # 3

saveRDS(model.1, &quot;output/LMMRmodel.RDS&quot;) # 4</code></pre>
<pre class="r"><code>model.1 &lt;- readRDS(file = &#39;output/LMMRmodel.RDS&#39;) #5</code></pre>
<pre class="r"><code>coefficients(model.1) # 6</code></pre>
<pre><code>## (Intercept)        X545        X555       X1505       X2195 
##    18.38680    75.38174   -78.80910    45.99251   -46.83051</code></pre>
<pre class="r"><code>best.bands &lt;- row.names(summary(model.1)$coefficients)[c(2, 3, 4, 5)] # 7

LMMR.model.eq &lt;- &quot;log[P/(1 - P)] = 18.387 + 75.382 log[R545] - 78.809 log[R555] + 45.993 log[R1505] - 46.831 log[R2195]&quot; # 8</code></pre>
<p>Based on the LMMR model we designed the LMMR index through mathematical simplification. We initially log transformed the spectral reflectance to apply Eq. 2 and 3 (Heim et al., 2018). Additionally, to summarize the model coefficients and yield 5/3 as a simplification, the absolute 95% confidence intervals for coefficient pairs should overlap as this indicates products of approximately equal magnitudes. Please refer to our article for the required steps yielding the LMMR.</p>
<pre class="r"><code>confint(model.1)</code></pre>
<pre><code>##                  2.5 %    97.5 %
## (Intercept)   9.964532  27.46334
## X545         57.012245  96.08337
## X555        -99.922625 -60.08532
## X1505        37.325040  56.02228
## X2195       -57.069230 -38.04457</code></pre>
<p><img src="code_files/figure-html/confintplot-1.png" width="672" /></p>
</div>
</div>
<div id="b" class="section level1">
<h1>B</h1>
<div id="create-spectral-library" class="section level2">
<h2>Create spectral library</h2>
<p>Initially, we load a spectral dataset (1), then we drop the class “Healthy” (2) and use our custom-build function (3) to create a spectral library for the <em>hsdar</em> package.</p>
<pre class="r"><code>spectra &lt;-
  read.csv(&quot;data/data.wo.out.binned.cut.csv&quot;, check.names = FALSE) %&gt;% #1
  drop_class(., .$Type, &quot;Healthy&quot;) %&gt;% #2
  raw2speclib(.) #3</code></pre>
</div>
<div id="define-spectral-vegetation-indices" class="section level2">
<h2>Define spectral vegetation indices</h2>
<p>We used the <em>hsdar</em> pkg to define spectral indices. The PRI and MCARI already existed in the package and we added the NBNDVI (1) and the LMMR (2). We stored them all in a vector to simplify processing (3).</p>
<pre class="r"><code>NBNDVI &lt;- &#39;(R850-R680)/(R850+R680)&#39; #1
LMMR &lt;- &#39;((R545/R555)^(5/3))*(R1505/R2195)&#39; #2

index &lt;- c(&quot;PRI&quot;, &quot;MCARI&quot;, NBNDVI, LMMR) #3</code></pre>
<p>After the indices were defined we can used them to convert our spectral data into index values by applying the indices on our spectral data (1-4). The LMMR must be transformed to the log scale (4) as it was developed that way and to use a fair comparison. We yield a new dataset (5) that contains index values and we add a column (6) containing the response (Treated, Untreated) for our classification.</p>
<pre class="r"><code>index.list &lt;- list()

index.list[[&#39;PRI&#39;]] &lt;- vegindex(spectra, index[1]) #1
index.list[[&#39;MCARI&#39;]] &lt;- vegindex(spectra, index[2]) #2
index.list[[&#39;NBNDVI&#39;]] &lt;- vegindex(spectra, index[3]) #3
index.list[[&#39;LMMR&#39;]] &lt;- log(vegindex(spectra, index[4])) #4

index.df &lt;- do.call(cbind.data.frame, index.list) #5
index.df$Type &lt;- Type #6

write.csv(index.df, &quot;output/specindices.csv&quot;, row.names = FALSE)</code></pre>
</div>
<div id="classification" class="section level2">
<h2>Classification</h2>
<p>First we split (1) our index dataset into training (2) and testing (3) to build a logistic regression model (train) and then validated this model on an isolated sample (test). We also defined classification parameters (4). For classification we used the <em>caret</em> pkg.</p>
<p>Splitting data:</p>
<pre class="r"><code>inTrain &lt;- createDataPartition(y = index.df$Type, p = .75, list = FALSE) #1

Train.75 &lt;- index.df[inTrain,] #2
Test.25 &lt;- index.df[-inTrain,] #3

ctrl &lt;- trainControl(method = &quot;boot&quot;,
                     number = 1000, 
                     classProbs = FALSE, 
                     savePredictions = TRUE) #4</code></pre>
<p>Creating training models:</p>
<pre class="r"><code># PRI Training
PRI.model &lt;- train(Type ~ PRI, 
                   data = Train.75, 
                   method = &quot;glm&quot;, 
                   trControl = ctrl, 
                   metric = c(&#39;Kappa&#39;))

PRIvalues &lt;- PRI.model$finalModel$fitted.values

sink(file = &quot;output/PRItrain75eval.txt&quot;)
PRI.model$pred
summary(PRI.model)# estimates
confusionMatrix(PRI.model)
sink()

# MCARI Training
MCARI.model &lt;- train(Type ~ MCARI,
                     data = Train.75, 
                     method = &quot;glm&quot;, 
                     trControl = ctrl, 
                     metric = c(&quot;Kappa&quot;))

MCARIvalues &lt;- MCARI.model$finalModel$fitted.values

sink(file = &quot;output/MCARItrain75eval.txt&quot;)
MCARI.model$pred
summary(MCARI.model)
confusionMatrix(MCARI.model)
sink()


# NBNDVI Training
NBNDVI.model &lt;- train(Type ~ NBNDVI,
                      data = Train.75,
                      method = &quot;glm&quot;,
                      trControl = ctrl,
                      metric = c(&quot;Kappa&quot;))

NBNDVIvalues &lt;- NBNDVI.model$finalModel$fitted.values

sink(file = &quot;output/NBNDVItrain75eval.txt&quot;)
NBNDVI.model$pred
summary(NBNDVI.model)
confusionMatrix(NBNDVI.model)
sink()


# LMMR Training
LMMR.model &lt;- train(Type ~ LMMR,
                    data = Train.75,
                    method = &quot;glm&quot;,
                    trControl = ctrl,
                    metric = c(&quot;Kappa&quot;))

LMMRvalues &lt;- LMMR.model$finalModel$fitted.values

sink(file = &quot;output/LMMRtrain75eval.txt&quot;)
LMMR.model$pred
summary(LMMR.model)
confusionMatrix(LMMR.model)
sink()</code></pre>
<p>Creating predictions (validation):</p>
<pre class="r"><code># PRI Testing
PRI.pred &lt;- predict(PRI.model, newdata = Test.25)

sink(&quot;output/PRITest25.txt&quot;, append=FALSE, split=FALSE)
confusionMatrix(data = PRI.pred, Test.25$Type)
sink()

# MCARI Testing
MCARI.pred &lt;- predict(MCARI.model, newdata = Test.25)

sink(&quot;output/MCARITest25.txt&quot;, append=FALSE, split=FALSE)
confusionMatrix(data = MCARI.pred, Test.25$Type)
sink()


# NBNDVI Testing
NBNDVI.pred &lt;- predict(NBNDVI.model, newdata = Test.25)

sink(&quot;output/NBNDVITest25.txt&quot;, append=FALSE, split=FALSE)
confusionMatrix(data = NBNDVI.pred, Test.25$Type)
sink()


# LMMR Testing
LMMR.pred &lt;- predict(LMMR.model, newdata = Test.25)

sink(&quot;output/LMMRTest25.txt&quot;, append=FALSE, split=FALSE)
confusionMatrix(data = LMMR.pred, Test.25$Type)
sink()</code></pre>
</div>
<div id="visualize-classification-training" class="section level2">
<h2>Visualize classification training</h2>
<p>First we built a df using the fitted values of the training process for each index (1-5) and added useful names for each column (6).</p>
<pre class="r"><code>train.res.df &lt;- as.data.frame(Train.75$Type) #1
train.res.df$PRIval &lt;- PRIvalues #2
train.res.df$MCARIval &lt;- MCARIvalues #3
train.res.df$NBNDVIval &lt;- NBNDVIvalues #4
train.res.df$LMMRval &lt;- LMMRvalues #5

names(train.res.df) &lt;- c(&#39;Type&#39;, &#39;PRI&#39;, &#39;MCARI&#39;, &#39;NBNDVI&#39;, &#39;LMMR&#39;) #6</code></pre>
<p>Then we can plot the results of the training process.<br />
<img src="code_files/figure-html/plottrain-1.png" width="672" /></p>
</div>
<div id="model-validation" class="section level2">
<h2>Model validation</h2>
<p>Below is the output of the model validation.</p>
</div>
<div id="section" class="section level2 tabset">
<h2></h2>
<div id="pri" class="section level3">
<h3>PRI</h3>
<pre><code>## Confusion Matrix and Statistics
## 
##            Reference
## Prediction  Treated Untreated
##   Treated        39        21
##   Untreated      20        36
##                                           
##                Accuracy : 0.6466          
##                  95% CI : (0.5524, 0.7331)
##     No Information Rate : 0.5086          
##     P-Value [Acc &gt; NIR] : 0.001876        
##                                           
##                   Kappa : 0.2927          
##  Mcnemar&#39;s Test P-Value : 1.000000        
##                                           
##             Sensitivity : 0.6610          
##             Specificity : 0.6316          
##          Pos Pred Value : 0.6500          
##          Neg Pred Value : 0.6429          
##              Prevalence : 0.5086          
##          Detection Rate : 0.3362          
##    Detection Prevalence : 0.5172          
##       Balanced Accuracy : 0.6463          
##                                           
##        &#39;Positive&#39; Class : Treated         
## </code></pre>
</div>
<div id="mcari" class="section level3">
<h3>MCARI</h3>
<pre><code>## Confusion Matrix and Statistics
## 
##            Reference
## Prediction  Treated Untreated
##   Treated        43        17
##   Untreated      16        40
##                                           
##                Accuracy : 0.7155          
##                  95% CI : (0.6243, 0.7954)
##     No Information Rate : 0.5086          
##     P-Value [Acc &gt; NIR] : 4.595e-06       
##                                           
##                   Kappa : 0.4307          
##  Mcnemar&#39;s Test P-Value : 1               
##                                           
##             Sensitivity : 0.7288          
##             Specificity : 0.7018          
##          Pos Pred Value : 0.7167          
##          Neg Pred Value : 0.7143          
##              Prevalence : 0.5086          
##          Detection Rate : 0.3707          
##    Detection Prevalence : 0.5172          
##       Balanced Accuracy : 0.7153          
##                                           
##        &#39;Positive&#39; Class : Treated         
## </code></pre>
</div>
<div id="nbndvi" class="section level3">
<h3>NBNDVI</h3>
<pre><code>## Confusion Matrix and Statistics
## 
##            Reference
## Prediction  Treated Untreated
##   Treated        49        32
##   Untreated      10        25
##                                           
##                Accuracy : 0.6379          
##                  95% CI : (0.5435, 0.7251)
##     No Information Rate : 0.5086          
##     P-Value [Acc &gt; NIR] : 0.003376        
##                                           
##                   Kappa : 0.2709          
##  Mcnemar&#39;s Test P-Value : 0.001194        
##                                           
##             Sensitivity : 0.8305          
##             Specificity : 0.4386          
##          Pos Pred Value : 0.6049          
##          Neg Pred Value : 0.7143          
##              Prevalence : 0.5086          
##          Detection Rate : 0.4224          
##    Detection Prevalence : 0.6983          
##       Balanced Accuracy : 0.6346          
##                                           
##        &#39;Positive&#39; Class : Treated         
## </code></pre>
</div>
<div id="lmmr" class="section level3">
<h3>LMMR</h3>
<pre><code>## Confusion Matrix and Statistics
## 
##            Reference
## Prediction  Treated Untreated
##   Treated        55         6
##   Untreated       4        51
##                                           
##                Accuracy : 0.9138          
##                  95% CI : (0.8472, 0.9579)
##     No Information Rate : 0.5086          
##     P-Value [Acc &gt; NIR] : &lt;2e-16          
##                                           
##                   Kappa : 0.8274          
##  Mcnemar&#39;s Test P-Value : 0.7518          
##                                           
##             Sensitivity : 0.9322          
##             Specificity : 0.8947          
##          Pos Pred Value : 0.9016          
##          Neg Pred Value : 0.9273          
##              Prevalence : 0.5086          
##          Detection Rate : 0.4741          
##    Detection Prevalence : 0.5259          
##       Balanced Accuracy : 0.9135          
##                                           
##        &#39;Positive&#39; Class : Treated         
## </code></pre>
</div>
</div>
<div id="visualize-spectra" class="section level2">
<h2>Visualize spectra</h2>
<p>Finally, we visualized the spectral signature for each classification group (Treated and Untreated) and showed the most important wavebands. First, we reformated our wide df into a long df (1). In a second step, we get the selected best bands (2) from a previous step and remove the X to have a numeric vector (3).</p>
<pre class="r"><code>specdat &lt;- read.csv(&quot;data/data.wo.out.binned.cut.csv&quot;, check.names = FALSE)
spectra.gg &lt;- prep_gg(specdat) #1

bands4gg &lt;-as.numeric(gsub(&#39;X&#39;, &#39;&#39;, best.bands)) #2, 3</code></pre>
<p>Now we can plot spectral signatures:</p>
<p><img src="code_files/figure-html/specplot-1.png" width="1800" /></p>
<p>Finally, we combined the last two plots, as shown in the article, and saved the plot:</p>
<pre class="r"><code># C) Get bar/jitter plots from 7A and modify to combine with 7B

p5 &lt;- plot_list[[2]]#MCARI
p5 &lt;- p5+
  theme(#axis.title.x=element_blank(),
      axis.text.x=element_blank(),
      axis.ticks.x=element_blank(),
      axis.title.y=element_blank())

p6 &lt;- plot_list[[3]]#NBNDVI
p6 &lt;- p6+
    theme(#axis.title.x=element_blank(),
      axis.text.x=element_blank(),
      axis.ticks.x=element_blank(),
      axis.title.y=element_blank())

p7 &lt;- plot_list[[4]]#LMMR
p7 &lt;- p7+
    theme(#axis.title.x=element_blank(),
      axis.text.x=element_blank(),
      axis.ticks.x=element_blank(),
      axis.title.y=element_blank())

p9 &lt;- plot_list[[1]]#PRI
p9 &lt;- p9+
    theme(#axis.title.x=element_blank(),
      axis.text.x=element_blank(),
      axis.ticks.x=element_blank())+
      labs(x = &quot;PRI&quot;, y=&quot;Disease Prob.&quot;)
  
# D) Build Figure 2
    
plot.res &lt;- ggdraw() +
    draw_plot(p9, x = 0, y = .5, width = .25, height = .5) +
    draw_plot(p5, x = .25, y = .5, width = .25, height = .5) +
    draw_plot(p6, x = .5, y = .5, width = .25, height = .5) +
    draw_plot(p7, x = .75, y = .5, width = .25, height = .5) +
      draw_plot(pspec, x = 0, y = 0, width = 1, height = 0.5) +
        draw_plot_label(label = c(&quot;A&quot;, &quot;B&quot;, &quot;C&quot;, &quot;D&quot;, &quot;E&quot;), size = 12,
                        x = c(0, 0.25, 0.5, 0.75, 0), y = c(1, 1, 1, 1, 0.5))

ggsave(&quot;output/Figure2.boxspectra.png&quot;,
    plot = plot.res,
    width = 40,
    height = 20,
    units = &quot;cm&quot;,
    dpi = 400
  )</code></pre>
</div>
</div>

<p>Copyright &copy; 2018 René Hans-Jürgen Heim</p>
contact: rene.heim@hdr.mq.edu.au


</div>
</div>

</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
